{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "keras0.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3I0Wlb8tr1b"
      },
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eqZy15hstwaq"
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtHjagHFty5q"
      },
      "source": [
        "x_train = x_train / 127.5 - 1\n",
        "x_test = x_test / 127.5 - 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bweyLE_Ft1vh"
      },
      "source": [
        "x_train.min(), x_train.max()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUYo8ZyZt4In"
      },
      "source": [
        "x_train = x_train.reshape(-1, 784)\n",
        "x_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQdUFzKVt5d9"
      },
      "source": [
        "from tensorflow.keras.layers import Dense, LeakyReLU, Dropout, Input\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.initializers import RandomNormal\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCpi1D2qt7uA"
      },
      "source": [
        "# gan에 입력되는 noise에 대한 dimension\n",
        "NOISE_DIM = 10\n",
        "\n",
        "# adam optimizer 정의, learning_rate = 0.0002, beta_1로 줍니다.\n",
        "# Vanilla Gan과 DCGAN에서 이렇게 셋팅을 해주는데\n",
        "# 이렇게 해줘야 훨씬 학습을 잘합니다.\n",
        "adam = Adam(lr=0.0002, beta_1=0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9bgFN5Xt_Ur"
      },
      "source": [
        "generator = Sequential([\n",
        "    Dense(256, input_dim=NOISE_DIM), \n",
        "    LeakyReLU(0.2), \n",
        "    Dense(28*28, activation='tanh'),\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FD_OQVMYxxm_"
      },
      "source": [
        "generator.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yDJmRHBx4_c"
      },
      "source": [
        "discriminator = Sequential([\n",
        "    Dense(256, input_shape=(784,), kernel_initializer=RandomNormal(stddev=0.02)),\n",
        "    LeakyReLU(0.2), \n",
        "    Dropout(0.3), \n",
        "    Dense(1, activation='sigmoid')\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WImqBFtIx61Z"
      },
      "source": [
        "discriminator.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdhVH9FXx8tC"
      },
      "source": [
        "discriminator.compile(loss='binary_crossentropy', optimizer=adam)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0IPkWw99x-Xp"
      },
      "source": [
        "# discriminator는 학습을 하지 않도록 하며, Gan 모델에서는 generator만 학습하도록 합니다.\n",
        "discriminator.trainable = False\n",
        "gan_input = Input(shape=(NOISE_DIM,))\n",
        "x = generator(inputs=gan_input)\n",
        "output = discriminator(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPIkNTK5x_5t"
      },
      "source": [
        "gan = Model(gan_input, output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ywI0MEtyB8I"
      },
      "source": [
        "gan.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-tBeHBm7yDVp"
      },
      "source": [
        "gan.compile(loss='binary_crossentropy', optimizer=adam)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpiYHdEwyHaI"
      },
      "source": [
        "def get_batches(data, batch_size):\n",
        "    batches = []\n",
        "    for i in range(int(data.shape[0] // batch_size)):\n",
        "        batch = data[i * batch_size: (i + 1) * batch_size]\n",
        "        batches.append(batch)\n",
        "    return np.asarray(batches)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNBhby07yIhA"
      },
      "source": [
        "def visualize_training(epoch, d_losses, g_losses):\n",
        "    '''\n",
        "    # 오차에 대한 시각화\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.plot(d_losses, label='Discriminator Loss')\n",
        "    plt.plot(g_losses, label='Generatror Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    print('epoch: {}, Discriminator Loss: {}, Generator Loss: {}'.format(epoch, np.asarray(d_losses).mean(), np.asarray(g_losses).mean()))\n",
        "    '''\n",
        "    #샘플 데이터 생성 후 시각화\n",
        "    noise = np.random.normal(0, 1, size=(24, NOISE_DIM))\n",
        "    generated_images = generator.predict(noise)\n",
        "    generated_images = generated_images.reshape(-1, 28, 28)\n",
        "    \n",
        "    plt.figure(figsize=(8, 4))\n",
        "    for i in range(generated_images.shape[0]):\n",
        "        plt.subplot(4, 6, i+1)\n",
        "        plt.imshow(generated_images[i], interpolation='nearest', cmap='gray')\n",
        "        plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Lc93zFhybHs"
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "EPOCHS= 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3ig449wykog"
      },
      "source": [
        "# discriminator와 gan 모델의 loss 측정을 위한 list 입니다.\n",
        "d_losses = []\n",
        "g_losses = []\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    start = time.time()\n",
        "\n",
        "    # 각 배치별 학습\n",
        "    for real_images in get_batches(x_train, BATCH_SIZE):\n",
        "        # 랜덤 노이즈 생성\n",
        "        input_noise = np.random.uniform(-1, 1, size=[BATCH_SIZE, NOISE_DIM])\n",
        "        \n",
        "        # 가짜 이미지 데이터 생성\n",
        "        generated_images = generator.predict(input_noise)\n",
        "        \n",
        "        # Gan에 학습할 X 데이터 정의\n",
        "        x_dis = np.concatenate([real_images, generated_images])\n",
        "        \n",
        "        # Gan에 학습할 Y 데이터 정의\n",
        "        y_dis = np.zeros(2 * BATCH_SIZE)\n",
        "        y_dis[:BATCH_SIZE] = 0.9\n",
        "        \n",
        "        # Discriminator 훈련\n",
        "        discriminator.trainable = True\n",
        "        d_loss = discriminator.train_on_batch(x_dis, y_dis)\n",
        "        \n",
        "        # Gan 훈련\n",
        "        noise = np.random.uniform(-1, 1, size=[BATCH_SIZE, NOISE_DIM])\n",
        "        y_gan = np.ones(BATCH_SIZE)\n",
        "        \n",
        "        # Discriminator의 판별 학습을 방지합니다\n",
        "        discriminator.trainable = False\n",
        "        g_loss = gan.train_on_batch(noise, y_gan)\n",
        "        \n",
        "    d_losses.append(d_loss)\n",
        "    g_losses.append(g_loss)\n",
        "    \n",
        "    if epoch == 1 or epoch % 10 == 0:\n",
        "        visualize_training(epoch, d_losses, g_losses)\n",
        "\n",
        "    # print (' 에포크 {} 에서 걸린 시간은 {} 초 입니다'.format(epoch +1, time.time()-start))\n",
        "    print ('Time for epoch {} is {} sec'.format(epoch, time.time()-start))   "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}